"""
ä¿®å¤åª’ä½“æ–‡ä»¶è¡¨ä¸­é‡å¤çš„file_hashé—®é¢˜
ç”¨äºæ¸…ç†ç°æœ‰çš„é‡å¤æ•°æ®
"""

import asyncio
import sys
import os
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tortoise import Tortoise
from settings.tortoise_config import TORTOISE_ORM
from api_versions.v2.models import MediaFile
from core.logger import logger


async def fix_duplicate_hashes():
    """ä¿®å¤é‡å¤çš„file_hashé—®é¢˜"""
    try:
        logger.info("ğŸš€ å¼€å§‹ä¿®å¤é‡å¤çš„file_hash...")
        
        # åˆå§‹åŒ–Tortoise ORM
        await Tortoise.init(config=TORTOISE_ORM)
        
        # æŸ¥æ‰¾æ‰€æœ‰é‡å¤çš„file_hash
        duplicate_hashes = await find_duplicate_hashes()
        
        if not duplicate_hashes:
            logger.info("âœ… æ²¡æœ‰å‘ç°é‡å¤çš„file_hash")
            return
        
        logger.info(f"ğŸ” å‘ç° {len(duplicate_hashes)} ä¸ªé‡å¤çš„file_hash")
        
        # å¤„ç†æ¯ä¸ªé‡å¤çš„hash
        for file_hash in duplicate_hashes:
            await fix_single_hash(file_hash)
        
        logger.info("âœ… é‡å¤file_hashä¿®å¤å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ ä¿®å¤é‡å¤file_hashå¤±è´¥: {str(e)}")
        raise e
    finally:
        # å…³é—­æ•°æ®åº“è¿æ¥
        await Tortoise.close_connections()


async def find_duplicate_hashes():
    """æŸ¥æ‰¾æ‰€æœ‰é‡å¤çš„file_hash"""
    # ä½¿ç”¨åŸç”ŸSQLæŸ¥è¯¢é‡å¤çš„hash
    conn = Tortoise.get_connection("default")
    
    query = """
    SELECT file_hash 
    FROM media_file 
    WHERE file_hash IS NOT NULL 
    GROUP BY file_hash 
    HAVING COUNT(*) > 1
    """
    
    result = await conn.execute_query(query)
    return [row[0] for row in result[1]]


async def fix_single_hash(file_hash: str):
    """ä¿®å¤å•ä¸ªé‡å¤çš„file_hash"""
    logger.info(f"ğŸ”§ å¤„ç†é‡å¤hash: {file_hash}")
    
    # è·å–æ‰€æœ‰å…·æœ‰ç›¸åŒhashçš„è®°å½•
    records = await MediaFile.filter(file_hash=file_hash).order_by("created_at")
    
    if len(records) <= 1:
        return
    
    # ä¿ç•™æœ€æ—©åˆ›å»ºçš„è®°å½•
    keep_record = records[0]
    duplicate_records = records[1:]
    
    logger.info(f"ä¿ç•™è®°å½•: ID={keep_record.id}, è·¯å¾„={keep_record.file_path}")
    
    # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦çœŸå®å­˜åœ¨
    static_dir = Path(__file__).parent.parent / "static"
    keep_file_path = static_dir / keep_record.file_path
    
    for dup_record in duplicate_records:
        dup_file_path = static_dir / dup_record.file_path
        
        # å¦‚æœä¿ç•™çš„æ–‡ä»¶ä¸å­˜åœ¨ï¼Œä½†é‡å¤çš„æ–‡ä»¶å­˜åœ¨ï¼Œåˆ™äº¤æ¢ä¿ç•™å¯¹è±¡
        if not keep_file_path.exists() and dup_file_path.exists():
            logger.info(f"äº¤æ¢ä¿ç•™è®°å½•: æ–°ä¿ç•™ID={dup_record.id}, è·¯å¾„={dup_record.file_path}")
            keep_record, dup_record = dup_record, keep_record
            keep_file_path = dup_file_path
        
        # æ ‡è®°é‡å¤è®°å½•ä¸ºåˆ é™¤
        logger.info(f"æ ‡è®°åˆ é™¤é‡å¤è®°å½•: ID={dup_record.id}, è·¯å¾„={dup_record.file_path}")
        await dup_record.update_from_dict({"is_delete": True}).save()
        
        # å¦‚æœé‡å¤è®°å½•çš„æ–‡ä»¶å­˜åœ¨ä½†ä¸ä¿ç•™è®°å½•è·¯å¾„ä¸åŒï¼Œåˆ é™¤é‡å¤æ–‡ä»¶
        if dup_file_path.exists() and dup_file_path != keep_file_path:
            try:
                dup_file_path.unlink()
                logger.info(f"åˆ é™¤é‡å¤æ–‡ä»¶: {dup_file_path}")
            except Exception as e:
                logger.warning(f"åˆ é™¤é‡å¤æ–‡ä»¶å¤±è´¥ {dup_file_path}: {e}")


async def cleanup_orphaned_records():
    """æ¸…ç†å­¤å„¿è®°å½•ï¼ˆæ–‡ä»¶ä¸å­˜åœ¨çš„è®°å½•ï¼‰"""
    logger.info("ğŸ§¹ å¼€å§‹æ¸…ç†å­¤å„¿è®°å½•...")
    
    static_dir = Path(__file__).parent.parent / "static"
    records = await MediaFile.filter(is_delete=False).all()
    
    orphaned_count = 0
    for record in records:
        file_path = static_dir / record.file_path
        if not file_path.exists():
            logger.info(f"æ ‡è®°å­¤å„¿è®°å½•ä¸ºåˆ é™¤: ID={record.id}, è·¯å¾„={record.file_path}")
            await record.update_from_dict({"is_delete": True}).save()
            orphaned_count += 1
    
    logger.info(f"âœ… æ¸…ç†äº† {orphaned_count} ä¸ªå­¤å„¿è®°å½•")


async def main():
    """ä¸»å‡½æ•°"""
    print("åª’ä½“æ–‡ä»¶é‡å¤hashä¿®å¤å·¥å…·")
    print("=" * 50)
    
    choice = input("è¯·é€‰æ‹©æ“ä½œ:\n1. ä¿®å¤é‡å¤hash\n2. æ¸…ç†å­¤å„¿è®°å½•\n3. å…¨éƒ¨æ‰§è¡Œ\nè¯·è¾“å…¥é€‰æ‹© (1/2/3): ")
    
    if choice == "1":
        await fix_duplicate_hashes()
    elif choice == "2":
        await Tortoise.init(config=TORTOISE_ORM)
        try:
            await cleanup_orphaned_records()
        finally:
            await Tortoise.close_connections()
    elif choice == "3":
        await fix_duplicate_hashes()
        await Tortoise.init(config=TORTOISE_ORM)
        try:
            await cleanup_orphaned_records()
        finally:
            await Tortoise.close_connections()
    else:
        print("æ— æ•ˆé€‰æ‹©")


if __name__ == "__main__":
    asyncio.run(main())
