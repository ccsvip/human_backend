import asyncio
import colorama
from fastapi import APIRouter, Request, Depends, HTTPException, status, UploadFile, File, Form
from fastapi.responses import StreamingResponse
from core.logger import logger
from utils.redis_tools import generate_cache_key, get_cached_sse_data, store_sse_bulk_data
from core.services.v2 import stt_server, llm_server, tts_server, llm_server_other
from core.redis_client import redis_client
import orjson
import time
from .schema import DeviceCreateSchema, DeviceUpdateSchema, AppWithKeySchema, Device_Pydantic, App_Pydantic, DeviceWithAppsSchema, MediaOutSchema, MediaOutWithURLSchema, MediaUpdateSchema
from .models import Device, App, MediaFile
from .statistics import router as statistics_router
from api_versions.device.routers import routers as device_router
from api_versions.env.routers import router as env_router
import uuid
import os
from pathlib import Path
from typing import Set
from datetime import datetime
import hashlib
from utils.media_sync import _cleanup_duplicates

router = APIRouter()

# åŒ…å«ç»Ÿè®¡è·¯ç”±
router.include_router(statistics_router, prefix="/statistics", tags=["ç»Ÿè®¡æ•°æ®"])

@router.post("/audio-to-text", description="æœ€å¤§æ”¯æŒ15MBçš„éŸ³é¢‘æ–‡ä»¶", summary="è¯­éŸ³è½¬æ–‡æœ¬æ¥å£")
async def audio_to_text(result:str=Depends(stt_server.audio_to_text)):
    return result

@router.post("/chat-messages-blocking", description="é˜»å¡æ¨¡å¼ llm", summary="é˜»å¡æ¨¡å¼ llm")
async def chat_messages_blocking(request:Request, text:str):
    from .statistics import record_chat_stats, record_online_user, record_response_time
    
    start_time = time.time()
    
    # è®°å½•åœ¨çº¿ç”¨æˆ·
    user_id = request.state.user_id or "anonymous"
    await record_online_user(user_id)
    
    result_json = await llm_server.chat_messages_block(request=request, text=text)
    
    # è®°å½•å¯¹è¯ç»Ÿè®¡å’Œå“åº”æ—¶é—´
    end_time = time.time()
    response_time = (end_time - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
    await record_chat_stats()
    await record_response_time(response_time)
    
    return result_json

@router.post("/chat-messages-streaming", description="æµæ¨¡å¼ llm", summary="æµæ¨¡å¼ llm", include_in_schema=False)
async def chat_messages_streaming(request:Request, text:str):
    # async for data in  llm_server.chat_messages_streaming(request=request, text=text):
    async for data in  llm_server.chat_messages_streaming_new(request=request, text=text):
        yield data

@router.get("/chat-messages-streaming", description="æµæ¨¡å¼ llm (GET)", summary="æµæ¨¡å¼ llm (GET)", include_in_schema=False)
async def chat_messages_streaming_get(request:Request, text:str):
    """GETæ–¹å¼çš„æµå¼èŠå¤©æ¥å£ï¼Œç”¨äºSSEè¿æ¥"""
    return StreamingResponse(
        llm_server.chat_messages_streaming_new(request=request, text=text),
        media_type='text/event-stream',
        headers={
            "Cache-Control": "no-cache",
            "Connection": "keep-alive",
            "Access-Control-Allow-Origin": "*",
            "Access-Control-Allow-Headers": "*"
        }
    )

@router.post("/text-to-audio", description="æ–‡æœ¬è½¬è¯­éŸ³æ¥å£", summary="æ–‡æœ¬è½¬è¯­éŸ³")
async def text_to_audio(request:Request, text:str):
    return await tts_server.text_to_audio_(request=request, text=text)

@router.post("/clear-context", description="æ¸…ç©ºç”¨æˆ·ä¸Šä¸‹æ–‡", summary="æ¸…ç©ºä¸Šä¸‹æ–‡")
async def clear_context(request: Request):
    """æ¸…ç©ºå½“å‰ç”¨æˆ·çš„å¯¹è¯ä¸Šä¸‹æ–‡"""
    try:
        api_key = request.state.api_key or ""
        user_id = request.state.user_id or ""

        if not user_id:
            raise HTTPException(status_code=400, detail="ç”¨æˆ·IDä¸èƒ½ä¸ºç©º")

        await llm_server.clear_user_context(api_key, user_id, "ç”¨æˆ·ä¸»åŠ¨æ¸…ç©º")

        return {"success": True, "message": "ä¸Šä¸‹æ–‡å·²æ¸…ç©º"}
    except Exception as e:
        logger.error(f"æ¸…ç©ºä¸Šä¸‹æ–‡å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"æ¸…ç©ºä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")

@router.get("/get-cached-questions", description="å¿…é¡»è¯·æ±‚å¤´ä¼ å…¥ dify_api_key reference_id user_id", summary="è·å–ç¼“å­˜é—®é¢˜åˆ—è¡¨")
async def get_cached_questions(request: Request, text: str = ""):
    """è·å–å½“å‰ç”¨æˆ·ç¼“å­˜çš„é—®é¢˜åˆ—è¡¨

    åªæœ‰å½“ç¼“å­˜ä¸­å­˜åœ¨ __END_OF_STREAM__ æ ‡è®°æ—¶ï¼Œæ‰è®¤ä¸ºç¼“å­˜æ˜¯å®Œæ•´çš„ï¼Œæ‰è¿”å›é—®é¢˜åˆ—è¡¨
    """
    try:
        api_key = request.state.api_key or ""
        user_id = request.state.user_id or ""
        reference_id = request.state.reference_id or ""

        if not user_id:
            raise HTTPException(status_code=400, detail="ç”¨æˆ·IDä¸èƒ½ä¸ºç©º")

        # è¯»å–â€œç”¨æˆ·é—®é¢˜åˆ—è¡¨â€ä½¿ç”¨çš„ä¸“ç”¨é”®ï¼šquestion:{api_key}:{user_id}:{reference_id}
        # ä¹‹å‰è¯¯ç”¨äº† SSE æµç¼“å­˜é”®ï¼ˆsse_cache:...hashï¼‰ï¼Œå¯¼è‡´å³ä½¿ Redis æœ‰æ•°æ®ä¹Ÿè¯»ä¸åˆ°é—®é¢˜åˆ—è¡¨
        question_cache_key = f"question:{api_key}:{user_id}:{reference_id}"
        cached_questions = await redis_client.lrange(question_cache_key, 0, -1)

        # å…¼å®¹æ—§é€»è¾‘ï¼šå¦‚æœæœªå‘½ä¸­ question: å‘½åç©ºé—´ï¼Œåˆ™å›é€€åˆ°å†å²çš„ sse_cache å“ˆå¸Œé”®
        used_cache_key = question_cache_key
        if not cached_questions:
            fallback_key = await generate_cache_key(request=request, text=text)
            fallback_data = await redis_client.lrange(fallback_key, 0, -1)
            if fallback_data:
                cached_questions = fallback_data
                used_cache_key = fallback_key

        # æ£€æŸ¥ç¼“å­˜æ˜¯å¦å®Œæ•´ï¼šå¿…é¡»åŒ…å« __END_OF_STREAM__ æ ‡è®°
        if cached_questions:
            # è°ƒè¯•ï¼šæ‰“å°ç¼“å­˜å†…å®¹ç±»å‹å’Œç»“æŸæ ‡è®°æ£€æŸ¥
            logger.info(f"ğŸ” ç¼“å­˜å†…å®¹ç±»å‹æ£€æŸ¥: {[type(item).__name__ for item in cached_questions[-3:]]}")
            logger.info(f"ğŸ” æœ€åå‡ é¡¹å†…å®¹: {cached_questions[-3:]}")

            # æ£€æŸ¥æ˜¯å¦åŒ…å«ç»“æŸæ ‡è®°ï¼ˆæ”¯æŒå­—ç¬¦ä¸²å’Œå­—èŠ‚ä¸¤ç§æ ¼å¼ï¼‰
            has_end_marker = (b"__END_OF_STREAM__" in cached_questions or
                            "__END_OF_STREAM__" in cached_questions)

            if has_end_marker:
                # è¿‡æ»¤æ‰ç»“æŸæ ‡è®°ï¼Œåªè¿”å›å®é™…çš„é—®é¢˜ï¼ˆæ”¯æŒå­—ç¬¦ä¸²å’Œå­—èŠ‚ä¸¤ç§æ ¼å¼ï¼‰
                filtered_questions = [q for q in cached_questions
                                    if q != b"__END_OF_STREAM__" and q != "__END_OF_STREAM__"]
                logger.info(f"âœ… ç¼“å­˜å®Œæ•´ï¼Œè¿”å›é—®é¢˜åˆ—è¡¨: {len(filtered_questions)} ä¸ªé—®é¢˜")
                return {
                    "success": True,
                    "questions": filtered_questions,
                    "count": len(filtered_questions),
                    "cache_key": used_cache_key
                }
            else:
                # ç¼“å­˜ä¸å®Œæ•´ï¼Œæµå¼å“åº”å¯èƒ½è¿˜åœ¨è¿›è¡Œä¸­
                logger.info(f"âš ï¸ ç¼“å­˜ä¸å®Œæ•´ï¼Œç¼ºå°‘ç»“æŸæ ‡è®°")
                return {
                    "success": False,
                    "message": "ç¼“å­˜ä¸å®Œæ•´ï¼Œæµå¼å“åº”å¯èƒ½è¿˜åœ¨è¿›è¡Œä¸­",
                    "questions": [],
                    "count": 0,
                    "cache_key": used_cache_key
                }
        else:
            return {
                "success": False,
                "message": "æœªæ‰¾åˆ°ç¼“å­˜çš„é—®é¢˜",
                "questions": [],
                "count": 0,
                "cache_key": used_cache_key
            }

    except Exception as e:
        logger.error(f"è·å–ç¼“å­˜é—®é¢˜åˆ—è¡¨å¤±è´¥: {e}")
        raise HTTPException(status_code=500, detail=f"è·å–ç¼“å­˜é—®é¢˜åˆ—è¡¨å¤±è´¥: {str(e)}")

@router.get("/parameters", description="å¿…é¡»åœ¨è¯·æ±‚å¤´ä¼ å…¥ dify_api_key", summary="è·å–å¼€åœºç™½å’Œå»ºè®®é—®é¢˜")
async def get_parameters(request:Request):
    return StreamingResponse(
        llm_server_other.parameters_(request=request),
        media_type='text/event-stream',
        headers={"X-Stream-Data": "true"}
    )

@router.post("/tts-blocking", description="é˜»å¡æ¨¡å¼ä¸»å…¥å£", summary="é€šè¿‡ä¼ é€’éŸ³é¢‘è·å–å…¨éƒ¨(é˜»å¡æ¨¡å¼)")
async def main_router(request:Request, text:str=Depends(stt_server.audio_to_text)):
    print(text)

    llm_result = await chat_messages_blocking(request=request, text=text)
    print(llm_result)

    tts_url = await text_to_audio(request=request, text=llm_result)
    print(tts_url)

    return {
        "question": text,
        "answer": llm_result,
        "url": tts_url
    }

# async def generate_stream(*, request, text):


#     # æµ‹è¯•ä¸å†™ç¼“å­˜çš„é€Ÿåº¦
#     # async for data in llm_server.chat_messages_streaming(request=request, text=text):
#     #     yield data

#     # async for parameter in llm_server_other.parameters_(request=request):
#         # yield parameter


#     # å†™ç¼“å­˜çš„é€Ÿåº¦

#     cache_key = await generate_cache_key(request=request, text=text)
#     cached_data = await get_cached_sse_data(request=request, cache_key=cache_key)

#     if cached_data:
#         logger.info(f"ğŸ¯ å‘½ä¸­SSEç¼“å­˜(å“ˆå¸Œ: {cache_key[-10:]})")
#         for data in cached_data:
#             yield data
#     else:
#         logger.info(f"ğŸ’¾ å¤„ç†æ–°çš„è¯·æ±‚å¹¶è¿›è¡Œç¼“å­˜(å“ˆå¸Œ: {cache_key[-10:]})")
#         async for data in llm_server.chat_messages_streaming(request=request, text=text):
#             yield data
#             await store_see_data_to_cache(request=request, cache_key=cache_key, sse_data=data)

#         async for parameter in llm_server_other.parameters_(request=request):
#             yield parameter
#             await store_see_data_to_cache(request=request, cache_key=cache_key, sse_data=parameter)

@router.get("/llm-streaming", description="æµæ¨¡å¼ llm", summary="æµæ¨¡å¼ llm")
async def llm_streaming(*, request:Request, text:str):
    text = text.strip("ï¼Ÿ?ã€‚.>")
    return StreamingResponse(
        generate_stream(request=request, text=text), 
        media_type='text/event-stream',
        headers={"X-Stream-Data": "true"}
    )
async def generate_stream(*, request, text, skip_question=False):
    cache_key = await generate_cache_key(request=request, text=text)
    cached_data = await get_cached_sse_data(request=request, cache_key=cache_key)
    if cached_data:
        logger.info(f"ğŸ¯  å‘½ä¸­SSEç¼“å­˜(å“ˆå¸Œ: {cache_key[-10:]})")
        for data in cached_data:
            if isinstance(data, str):
                data = data.encode('utf-8')
            if skip_question and b'"event": "message"' in data and b'"question":' in data:
                continue
            yield data
    else:
        if text:
            logger.info(f"ğŸ’¾  å¤„ç†æ–°çš„è¯·æ±‚å¹¶è¿›è¡Œç¼“å­˜(å“ˆå¸Œ: {cache_key[-10:]})")
            buffer = []
            buffer_size = 10

            # async for data in llm_server.chat_messages_streaming(request=request, text=text, skip_question=skip_question):
            async for data in llm_server.chat_messages_streaming_new(request=request, text=text, skip_question=skip_question):
                if isinstance(data, str):
                    data_bytes = data.encode('utf-8')
                else:
                    data_bytes = data
                yield data_bytes
                buffer.append(data_bytes)

                if len(buffer) >= buffer_size:
                    asyncio.create_task(store_sse_bulk_data(cache_key, buffer.copy()))
                    buffer.clear()

            if buffer:
                await store_sse_bulk_data(cache_key, buffer)
                buffer.clear()
            param_buffer = []
            async for parameter in llm_server_other.parameters_(request=request):
                if isinstance(parameter, str):
                    parameter_bytes = parameter.encode('utf-8')
                else:
                    parameter_bytes = parameter
                yield parameter_bytes
                param_buffer.append(parameter_bytes)
            if param_buffer:
                await store_sse_bulk_data(cache_key, param_buffer, append=True)
                await redis_client.rpush(cache_key, b"__END_OF_STREAM__")
        else:
            # ä¸å†™ç¼“å­˜
            # async for data in llm_server.chat_messages_streaming(request=request, text=text, skip_question=skip_question):
            async for data in llm_server.chat_messages_streaming_new(request=request, text=text, skip_question=skip_question):
                if isinstance(data, str):
                    data_bytes = data.encode('utf-8')
                else:
                    data_bytes = data
                yield data_bytes
            
            async for parameter in llm_server_other.parameters_(request=request):
                if isinstance(parameter, str):
                    parameter_bytes = parameter.encode('utf-8')
                else:
                    parameter_bytes = parameter
                yield parameter_bytes

@router.post("/tts", description="æµæ¨¡å¼ä¸»å…¥å£", summary="é€šè¿‡ä¼ é€’éŸ³é¢‘è·å–å…¨éƒ¨(æµæ¨¡å¼)")
async def main_router_streaming(request: Request, text: str = Depends(stt_server.audio_to_text)):
    async def stream_generator():
        if text:
            cache_key = await generate_cache_key(request=request, text=text)
            cached_data = await get_cached_sse_data(request=request, cache_key=cache_key)
            if cached_data:
                logger.info(f"ğŸ¯  å‘½ä¸­å®Œæ•´SSEç¼“å­˜(å“ˆå¸Œ: {cache_key[-10:]})")
                for data in cached_data:
                    if isinstance(data, str):
                        data = data.encode('utf-8')
                    if b'"event": "message"' in data and b'"question":' in data:
                        continue
                    yield data
                return
        async for data in generate_stream(request=request, text=text, skip_question=False):
            yield data
    return StreamingResponse(
        stream_generator(),
        media_type='text/event-stream',
        headers={"X-Stream-Data": "true"}
    )